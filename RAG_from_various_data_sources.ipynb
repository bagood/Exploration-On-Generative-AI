{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f311b54-10ce-4d09-a2c2-4e71837ef6f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --user langchain python-dotenv google-generativeai langchain-google-genai langchain-community youtube-transcript-api chromadb==0.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10bf16-4e40-41ac-b183-98f82ba686ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r chroma_db/chroma_db_pdf\n",
    "!mkdir chroma_db/chroma_db_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624ea72-957a-461f-a9ca-4660f66e464d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY='INSERT THE GOOGLE API KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825d50e-1321-4728-ac77-5b0eab0ab116",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Simple Prompt Using Gemini + Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0029411-bb71-45fd-9e18-2286961a1c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# utilize gemini as the model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# create query for the model\n",
    "# the query contains short instructions and a question\n",
    "# the question is stored in the variable \"topic\"\n",
    "prompt = PromptTemplate.from_template(\"You are a content student of mathematics. Create a simple explanation about {query}\")\n",
    "\n",
    "# create a runnable interface to make query to the model\n",
    "prompt_chain = LLMChain(llm=llm, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61442ba7-a236-496b-a058-2eb5056f4f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"how does linear function works\"\n",
    "\n",
    "# make a query to the model\n",
    "resp = prompt_chain.run(query=query) \n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a9158-50d3-4c4c-ab9e-0b5b3e419af1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8d4b2-8864-4077-9604-220e948883bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create RAG Prompt Using Gemini + Langchain to Answer Questions Based on Youtube Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13d737-a5bc-4222-a3b2-a2c496895a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f97625-d279-497b-90fa-06d584ae07f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_retriever_from_youtube_video_url(video_url, k):\n",
    "    # retrieve the transcript of the youtube video\n",
    "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "    transcript = loader.load()\n",
    "\n",
    "    # splits the transcript into several smaller documents, each containing chunks of the transcript\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(transcript)\n",
    "    \n",
    "    # embed the documents and store it in a vector database\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "                     documents=docs,                 # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"chroma_db/chroma_db_youtube\" # Directory to save data\n",
    "                     )\n",
    "    vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"chroma_db/chroma_db_youtube\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "    \n",
    "    # a vector store retriever to retrieve the embedded documents\n",
    "    retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def get_response_from_query(retriever, query):\n",
    "    # retrieve documents that has high similiarity with the given query\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # create an instance of the gemini model\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # prompt text for the model \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"docs\"],\n",
    "        template=\"\"\"\n",
    "        You are a helpful assistant that can answer questions about youtube videos based on the video's transcript.\n",
    "        \n",
    "        Answer the following question: {query}\n",
    "        By searching the following video transcript: {docs}\n",
    "        \n",
    "        Only use the factual information from the transcript to answer the question.\n",
    "        \n",
    "        If you feel like you don't have enough information to answer the question, say \"I don't know\".\n",
    "        \n",
    "        Your answers should be verbose and detailed.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # chain of steps to make rag prompt\n",
    "    rag_chain = (\n",
    "        {\"docs\": retriever | format_docs, # documents used inside the prompt are acquired from the retriever and formatted with the funcrion \"format_docs\"\n",
    "         \"query\": RunnablePassthrough()} # the query is provided in the input through applying the method of \"invoke\"\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    ) \n",
    "    \n",
    "    # make prompt based on the chain created\n",
    "    resp = rag_chain.invoke(query)\n",
    "\n",
    "    return resp, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c453c-566d-4a63-b1b6-71d6a92509a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_url = 'https://www.youtube.com/watch?v=yLsygYydHas'\n",
    "k = 1\n",
    "\n",
    "retriever = create_retriever_from_youtube_video_url(video_url, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e727e5-c20d-4e72-95df-c8fe0d55e649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp, docs = get_response_from_query(retriever, 'Tell me something about mathematics')\n",
    "print(resp, '\\n')\n",
    "[print(d, '\\n') for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b09c139-6954-432a-8b6f-a3f528a4002c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp, docs = get_response_from_query(retriever, \"Why does the house of the dragon showrunner talks about daemon's visions\")\n",
    "print(resp, '\\n')\n",
    "[print(d, '\\n') for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7cc05-0be1-4099-bc18-512c67e24c90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create RAG Prompt Using Gemini + Langchain to Answer Questions Based on CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962b735-b108-40d6-b4e7-9cbec439aa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceedbfa-b227-45af-b025-2d142c81bc64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_retriever_from_csv_files(csv_file_path, k):\n",
    "    # load csv file and stored it in a variable\n",
    "    loader = CSVLoader(file_path=csv_file_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # embed the documents and store it in a vector database\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "                         documents=docs,                 # Data\n",
    "                         embedding=gemini_embeddings,    # Embedding model\n",
    "                         persist_directory=\"chroma_db/chroma_db_csv_files\" # Directory to save data\n",
    "                         )\n",
    "    vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"chroma_db/chroma_db_csv_files\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "\n",
    "    # a vector store retriever to retrieve the embedded documents\n",
    "    retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content.split('Answer: ')[1] for d in docs)\n",
    "\n",
    "def get_response_from_query(retriever, query):\n",
    "    # retrieve documents that has high similarity with the given query\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # create an instance of the gemini model\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # prompt text for the model \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"docs\"],\n",
    "        template=\"\"\"\n",
    "        You are a helpful assistant that can answer questions about Ariabagus's profile\n",
    "        based on the informations provided by Ariabagus.\n",
    "        \n",
    "        Answer the following question: {query}\n",
    "        By searching through these informations: {docs}\n",
    "        \n",
    "        Only use the factual information from the provided informations to answer the question.\n",
    "        \n",
    "        If you feel like you don't have enough information to answer the question, say \"I don't know\".\n",
    "        \n",
    "        Your answers should be verbose and detailed.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # chain of steps to make rag prompt\n",
    "    rag_chain = (\n",
    "        {\"docs\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # make prompt based on the chain created\n",
    "    resp = rag_chain.invoke(query)\n",
    "\n",
    "    return resp, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ff5ff-829d-4ab4-bc5e-4f4a320d2dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = create_retriever_from_csv_files('train_data.csv', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14421ffc-b3d5-4df5-9f63-f3cbaacb2dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp, docs = get_response_from_query(retriever, 'What is your name?')\n",
    "print(resp, '\\n')\n",
    "[print(d, '\\n') for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba16bd-ea57-419a-9f39-7c5620a72d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp, docs = get_response_from_query(retriever, 'Which university do you go for study?')\n",
    "print(resp, '\\n')\n",
    "[print(d, '\\n') for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e229a20-d788-4f28-99d8-fe61ba6ce088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp, docs = get_response_from_query(retriever, 'What is your name and at which university do you go for study?')\n",
    "print(resp, '\\n')\n",
    "[print(d, '\\n') for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561a219-04bf-4e0e-b3a7-dc7ac29ae701",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create RAG Prompt Using Gemini + Langchain to Answer Questions Based on Data From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87139a34-88c9-454f-a3da-f88f4535d5d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff653a0-9e47-4e0f-a1a5-5eb4d4ce5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset_into_proper_format(row):\n",
    "    instruction = row['instruction']\n",
    "    input_val = row['input']\n",
    "    output_val = row['output']\n",
    "\n",
    "    reformatted_prompt = f\"\"\"Instruction:{instruction}\\n\\nInput:{input_val}\\n\\nOutput:{output_val}\"\"\"\n",
    "    \n",
    "    return {'prompt': reformatted_prompt}\n",
    "\n",
    "def convert_huggingface_data_to_documents(dataset_name):\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, split=\"train\").shuffle(seed=42) \\\n",
    "                                                            .select(range(1000)) \\\n",
    "                                                            .map(transform_dataset_into_proper_format)\n",
    "    \n",
    "    # convert data into documents type of data\n",
    "    docs = [LangchainDocument(page_content=doc[\"prompt\"], metadata={\"index\": i}) for i, doc in enumerate(tqdm(dataset))]\n",
    "                                                                    \n",
    "    return docs\n",
    "\n",
    "def create_retriever_from_documents(docs, k):\n",
    "    # embed the documents and store it in a vector database\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "                         documents=docs,                 # Data\n",
    "                         embedding=gemini_embeddings,    # Embedding model\n",
    "                         persist_directory=\"chroma_db/chroma_db_huggingface\" # Directory to save data\n",
    "                         )\n",
    "    vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"chroma_db/chroma_db_huggingface\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "\n",
    "    # a vector store retriever to retrieve the embedded documents\n",
    "    retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content.split('Output:')[1] for d in docs)\n",
    "\n",
    "def get_response_from_query(retriever, query):\n",
    "    # retrieve documents that has high similarity with the given query\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # create an instance of the gemini model\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # prompt text for the model \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"docs\"],\n",
    "        template=\"\"\"\n",
    "        You are a helpful assistant that can generate a Python code to solve certain task\n",
    "        utilizing the given Python codes that solves similar task\n",
    "        \n",
    "        Generate a Python code that solves this task: {query}\n",
    "        Utilizing informations acquired from these Python Codes: {docs}\n",
    "        \n",
    "        Utilize these informations to develop the desired Python code\n",
    "        \n",
    "        If you feel like you don't have enough information to answer the question, say \"I don't know\"\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # chain of steps to make rag prompt\n",
    "    rag_chain = (\n",
    "        {\"docs\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # make prompt based on the chain created\n",
    "    resp = rag_chain.invoke(query)\n",
    "\n",
    "    return resp, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93719a-dbc4-424c-b45b-668ba8387788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "docs = convert_huggingface_data_to_documents(dataset_name)\n",
    "retriever = create_retriever_from_documents(docs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5b74e-246a-4afe-be76-291c356102b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = 'Create a for loop in Python that prints the output of a multiplication table for numbers from 1 to 10.'\n",
    "resp, docs = get_response_from_query(retriever, query)\n",
    "print(resp, '\\n')\n",
    "[print(d, '\\n') for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73872f54-ecfb-42cd-9dc2-78a82df52b13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create RAG Prompt Using Gemini + Langchain to Answer Questions Based on Data From PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e7839-e776-4b7c-9510-d3563ff637d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee5fbc-79e8-4c82-a68b-382ffcd2e657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_retriever_from_pdf_file(pdf_filename, k):\n",
    "    loader = PyPDFLoader(pdf_filename)\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # splits the texts into several smaller documents, each containing chunks of the texts\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    \n",
    "    # embed the documents and store it in a vector database\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "                     documents=docs,                 # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"chroma_db/chroma_db_pdf\" # Directory to save data\n",
    "                     )\n",
    "    vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"chroma_db/chroma_db_pdf\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "    \n",
    "    # a vector store retriever to retrieve the embedded documents\n",
    "    retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def get_response_from_query(retriever, query):\n",
    "    # retrieve documents that has high similiarity with the given query\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # create an instance of the gemini model\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # prompt text for the model \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"docs\"],\n",
    "        template=\"\"\"\n",
    "        You are a helpful assistant that can answer questions about machine learning.\n",
    "        \n",
    "        Answer the following question: {query}\n",
    "        By searching the following informations: {docs}\n",
    "        \n",
    "        Only use the factual information from the given information to answer the question.\n",
    "        \n",
    "        If you feel like you don't have enough information to answer the question, say \"I don't know\".\n",
    "        \n",
    "        Your answers should be verbose and detailed.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # chain of steps to make rag prompt\n",
    "    rag_chain = (\n",
    "        {\"docs\": retriever | format_docs, # documents used inside the prompt are acquired from the retriever and formatted with the funcrion \"format_docs\"\n",
    "         \"query\": RunnablePassthrough()} # the query is provided in the input through applying the method of \"invoke\"\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    ) \n",
    "    \n",
    "    # make prompt based on the chain created\n",
    "    resp = rag_chain.invoke(query)\n",
    "\n",
    "    return resp, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fcd43-fb7b-4b3b-8443-c99702c12b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_filename = \"FILENAME\"\n",
    "retriever = create_retriever_from_pdf_file(pdf_filename, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b1413-ebe0-4ea8-96ec-3e575a9847ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp, docs = get_response_from_query(retriever, 'What is machine learning')\n",
    "print(resp, '\\n')\n",
    "[print(d, '\\n') for d in docs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
